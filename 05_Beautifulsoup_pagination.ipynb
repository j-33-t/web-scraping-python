{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pagination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"https://subslikescript.com/movies\"\n",
    "result = requests.get(website)\n",
    "content = result.text\n",
    "soup = BeautifulSoup(content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagination = soup.find('ul', class_= \"pagination\")\n",
    "pages = pagination.find_all('li', class_=\"page-item\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract first 5 movie titles from 5 pages on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = []  # Store all links from each page\n",
    "page_links = []  # Store links from a single page\n",
    "\n",
    "for pageNumber in range(5):\n",
    "    url = f\"https://subslikescript.com/movies?page={pageNumber}\"\n",
    "    result = requests.get(url)\n",
    "    content = result.text\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    box = soup.find('article', class_=\"main-article\")\n",
    "    count = 0  # Counter for number of links appended\n",
    "    for link in box.find_all('a', href=True):\n",
    "        page_links.append(f\"https://subslikescript.com/{link['href']}\")\n",
    "        count += 1\n",
    "        if count == 5:  # Break the loop after appending 5 links\n",
    "            break\n",
    "    all_links.append(page_links)  # Append links from current page to all_links\n",
    "    page_links = []  # Reset page_links for the next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Number: 1\n",
      "A vörös grófnö (1985) - full transcript\n",
      "Cool It, Carol! (1970) - full transcript\n",
      "Los cuatro secretos (1976) - full transcript\n",
      "Hellfighters (1968) - full transcript\n",
      "Age of Nudity (1959) - full transcript\n",
      "\n",
      "\n",
      "Page Number: 2\n",
      "A vörös grófnö (1985) - full transcript\n",
      "Cool It, Carol! (1970) - full transcript\n",
      "Los cuatro secretos (1976) - full transcript\n",
      "Hellfighters (1968) - full transcript\n",
      "Age of Nudity (1959) - full transcript\n",
      "\n",
      "\n",
      "Page Number: 3\n",
      "The Neighbor (2022) - full transcript\n",
      "2018 (2023) - full transcript\n",
      "Skeleton Flower (2021) - full transcript\n",
      "Kanojo no sukinamonowa (2021) - full transcript\n",
      "The Incredibly True Adventure of Two Girls in Love (1995) - full transcript\n",
      "\n",
      "\n",
      "Page Number: 4\n",
      "Zivot a neobycejna dobrodruzstvi vojaka Ivana Conkina (1994) - full transcript\n",
      "The Stones and Brian Jones (2023) - full transcript\n",
      "He Who Must Die (1957) - full transcript\n",
      "Never on Sunday (1960) - full transcript\n",
      "Sisu (2019) - full transcript\n",
      "\n",
      "\n",
      "Page Number: 5\n",
      "Srimannarayana (2012) - full transcript\n",
      "The Last of Robin Hood (2013) - full transcript\n",
      "Brooklyn 45 (2023) - full transcript\n",
      "Flamin' Hot (2023) - full transcript\n",
      "Custody (2023) - full transcript\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The following code is to iterate over all the links stored in object \"all_links\"\n",
    "# Each iteration will print the requested element for each link on a page\n",
    "for i in range(5):\n",
    "    print(f\"Page Number: {i+1}\")\n",
    "    for j in range(5):\n",
    "        website = all_links[i][j]\n",
    "        result = requests.get(website)\n",
    "        content = result.text\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        box = soup.find('article', class_=\"main-article\")\n",
    "        \n",
    "        # Check if the content box is found on the webpage\n",
    "        if box is None:\n",
    "            print(\"Error: Content not found\")\n",
    "            continue\n",
    "        \n",
    "        # Find the title element within the content box\n",
    "        title_element = box.find('h1')\n",
    "        \n",
    "        # Check if the title element is found\n",
    "        if title_element is None:\n",
    "            print(\"Error: Title element not found\")\n",
    "            continue\n",
    "        \n",
    "        # Print the title text\n",
    "        print(title_element.get_text())\n",
    "        \n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ScrapingVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
